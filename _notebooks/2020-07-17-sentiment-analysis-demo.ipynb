{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Tweets using BERT\n",
    "> In this notebook we will go through the process of classiying tweets(or any text data for that matter) into positive,negative or neutral.\n",
    "The dataset we use for this task is the [Airline Tweets Dataset](https://www.kaggle.com/crowdflower/twitter-airline-sentiment)\n",
    "\n",
    "> We will be using [MLFlow](https://mlflow.org/) to track our traininig process.\n",
    "\n",
    "> If you are not running it via a jupyterhub image but locally or by cloning the repository,to set up the environment please refer to this [doc](https://docs.google.com/document/d/1BUEzAeymOr1NyWQT4_vY22dFlMinjcbeV6iFBZhBTYY/edit) and the requirements.txt in the repository\n",
    "\n",
    "\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [fastpages, jupyter, sentimentanalysis, machinelearning, naturallanguageprocessing, deeplearing, interpret-text, interpretability, bert]\n",
    "- hide: false\n",
    "- search_exclude: true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "Sentiment Analysis is the automated process of analyzing text data and sorting it into sentiments depending on the problem statement. The ability to extract insights from this type of data is a practice that is widely adopted by many organisations across the world. Its applications are broad and powerful. A very important use case for sentiment analysis is brand reputation management.\n",
    "\n",
    "Red Hat has a variety of text based artifacts coming from sources starting from partner and customer engagements to documentation and communication logs. These text based artifacts are valuable and can be used to generate business insights and inform decisions if appropriately mined. The goal of this project is to allow other teams across Red Hat to have a tool at their disposal allowing them to analyze their text data and make informed decisions based on the insights gained from them. In this blog post we take a public dataset as an example to walk through the work flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "The goal is to make a deep learning model which can classify emotion in a given sentence.\n",
    "\n",
    "We do this by making use of transfer learning on the BERT model architecture. So that this can be used as a sample workflow, we take publicly available data as an example, as the original workflow consists of sensitive data. We also discuss interpreting BERT using the Unified Information Explainer algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "Given a text our goal is to predict whether it conveys a positive, negative or neutral emotion. Hence we want to build a text classifier for our data. There are various approaches to perform this task but for our project we pick the approach used in most state-of-the-art textual analysis systems i.e. deep learning.\n",
    "\n",
    "To construct a deep learning model which is very accurate we require huge amounts of data and compute resources. But luckily for us models like [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) are pre trained on large amounts of data and made publicly available. Therefore we can fine tune an already pre trained model like BERT on our own data to leverage what the model has already learnt. This process is called [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will download the pre trained model and files required which allow us to use it easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-07-17 16:28:07--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.13.80, 172.217.13.240, 172.217.12.240, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.13.80|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 407727028 (389M) [application/zip]\n",
      "Saving to: ‘models/bert/uncased_L-12_H-768_A-12.zip’\n",
      "\n",
      "43% [===============>                       ] 176,168,960 88.8MB/s             ^C\n"
     ]
    }
   ],
   "source": [
    "#collapse-hide\n",
    "\n",
    "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip -P models/bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import gc\n",
    "import os\n",
    "import fileinput\n",
    "import string\n",
    "import zipfile\n",
    "import datetime\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "import mlflow\n",
    "from pandas import DataFrame\n",
    "sys.path.insert(0, 'models/bert')\n",
    "from models.bert import modeling\n",
    "from models.bert import optimization\n",
    "from models.bert import run_classifier\n",
    "from models.bert import tokenization\n",
    "\n",
    "#extracting the downloaded model\n",
    "folder = 'models/bert'\n",
    "with zipfile.ZipFile(\"models/bert/uncased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we initalize the MLFlow client in the following step so that we can track our run and the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_CLIENT = mlflow.tracking.MlflowClient(tracking_uri='http://mlflow-server-route-aiops-prod-prometheus-scrape.cloud.paas.psi.redhat.com')\n",
    "mlflow.set_tracking_uri(\"http://mlflow-server-route-aiops-prod-prometheus-scrape.cloud.paas.psi.redhat.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment('sentiment_analysis_test_0.1')\n",
    "mlflow.start_run(run_name=\"airline_tweets-trialrun-same-artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_run_id = mlflow.active_run().info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "64f2c3a0c296d5bb724b405d18a8df438db444da"
   },
   "source": [
    "# BERT implementation\n",
    "\n",
    "We are going to use Google's pre trained BERT for our classification tasks. \n",
    "Apart from the model itself we also directly use Google's scripts to run our classifier which enables us to use the model for our data specifically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset\n",
    "\n",
    "For our demo we make use of the Twitter US Airline Sentiment public [dataset](https://www.kaggle.com/crowdflower/twitter-airline-sentiment) . This dataset consists of tweets directed at six US airlines with each of them classified into neutral, positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Cleaning data \n",
    "\n",
    "First we load up our data in the csv format into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "1aa95f622d1b59645a9e22c2e6d5d2a5f29a2a50"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>569940323746516993</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TaylorLumsden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica first time flying you all. do y...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-23 11:22:16 -0800</td>\n",
       "      <td>Dallas, Texas</td>\n",
       "      <td>Mountain Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9418</th>\n",
       "      <td>569943857418399746</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>0.6772</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thomashoward88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@USAirways US 728 stated their issues as: no o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-23 11:36:19 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8703</th>\n",
       "      <td>567935527481188352</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Delta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ToTravelToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@JetBlue Anywhere warm cause its freezing in NYC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-17 22:35:56 -0800</td>\n",
       "      <td>NYC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>569941957490774016</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TaylorLumsden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica awesome. I flew yall Sat mornin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-23 11:28:46 -0800</td>\n",
       "      <td>Dallas, Texas</td>\n",
       "      <td>Mountain Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13267</th>\n",
       "      <td>569900784965554176</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.7123</td>\n",
       "      <td>Flight Booking Problems</td>\n",
       "      <td>0.7123</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>milz02315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir Can you add my KTN to an existing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-23 08:45:10 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12962</th>\n",
       "      <td>569972985521532929</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T_Lubinski</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir Trying to get my flight changed t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-23 13:32:03 -0800</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11773</th>\n",
       "      <td>567767738886545408</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>NaN</td>\n",
       "      <td>izzyflan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@USAirways never in my life have I dealt with ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-17 11:29:12 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9205</th>\n",
       "      <td>570068659193950208</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EverettWJones</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@USAirways thank you.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-23 19:52:14 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Quito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>569853646411661314</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Late Flight</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>United</td>\n",
       "      <td>NaN</td>\n",
       "      <td>crog</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>.@united You may \"dislike delays\" but I paid y...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-23 05:37:51 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mountain Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11899</th>\n",
       "      <td>570304633048047616</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6529</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AesaGaming</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir Do you have any sort of live chat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:29:54 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "77     569940323746516993           neutral                        1.0000   \n",
       "9418   569943857418399746          negative                        1.0000   \n",
       "8703   567935527481188352           neutral                        1.0000   \n",
       "75     569941957490774016          positive                        1.0000   \n",
       "13267  569900784965554176          negative                        0.7123   \n",
       "12962  569972985521532929          negative                        1.0000   \n",
       "11773  567767738886545408          negative                        1.0000   \n",
       "9205   570068659193950208           neutral                        1.0000   \n",
       "1283   569853646411661314          negative                        1.0000   \n",
       "11899  570304633048047616           neutral                        0.6529   \n",
       "\n",
       "                negativereason  negativereason_confidence         airline  \\\n",
       "77                         NaN                        NaN  Virgin America   \n",
       "9418    Customer Service Issue                     0.6772      US Airways   \n",
       "8703                       NaN                        NaN           Delta   \n",
       "75                         NaN                        NaN  Virgin America   \n",
       "13267  Flight Booking Problems                     0.7123        American   \n",
       "12962   Customer Service Issue                     1.0000        American   \n",
       "11773   Customer Service Issue                     1.0000      US Airways   \n",
       "9205                       NaN                        NaN      US Airways   \n",
       "1283               Late Flight                     1.0000          United   \n",
       "11899                      NaN                     0.0000        American   \n",
       "\n",
       "      airline_sentiment_gold            name negativereason_gold  \\\n",
       "77                       NaN   TaylorLumsden                 NaN   \n",
       "9418                     NaN  thomashoward88                 NaN   \n",
       "8703                     NaN  ToTravelToLive                 NaN   \n",
       "75                       NaN   TaylorLumsden                 NaN   \n",
       "13267                    NaN       milz02315                 NaN   \n",
       "12962                    NaN      T_Lubinski                 NaN   \n",
       "11773                    NaN        izzyflan                 NaN   \n",
       "9205                     NaN   EverettWJones                 NaN   \n",
       "1283                     NaN            crog                 NaN   \n",
       "11899                    NaN      AesaGaming                 NaN   \n",
       "\n",
       "       retweet_count                                               text  \\\n",
       "77                 0  @VirginAmerica first time flying you all. do y...   \n",
       "9418               0  @USAirways US 728 stated their issues as: no o...   \n",
       "8703               0   @JetBlue Anywhere warm cause its freezing in NYC   \n",
       "75                 0  @VirginAmerica awesome. I flew yall Sat mornin...   \n",
       "13267              0  @AmericanAir Can you add my KTN to an existing...   \n",
       "12962              0  @AmericanAir Trying to get my flight changed t...   \n",
       "11773              0  @USAirways never in my life have I dealt with ...   \n",
       "9205               0                              @USAirways thank you.   \n",
       "1283               0  .@united You may \"dislike delays\" but I paid y...   \n",
       "11899              0  @AmericanAir Do you have any sort of live chat...   \n",
       "\n",
       "      tweet_coord              tweet_created tweet_location  \\\n",
       "77            NaN  2015-02-23 11:22:16 -0800  Dallas, Texas   \n",
       "9418          NaN  2015-02-23 11:36:19 -0800            NaN   \n",
       "8703          NaN  2015-02-17 22:35:56 -0800            NYC   \n",
       "75            NaN  2015-02-23 11:28:46 -0800  Dallas, Texas   \n",
       "13267         NaN  2015-02-23 08:45:10 -0800            NaN   \n",
       "12962         NaN  2015-02-23 13:32:03 -0800     Boston, MA   \n",
       "11773         NaN  2015-02-17 11:29:12 -0800            NaN   \n",
       "9205          NaN  2015-02-23 19:52:14 -0800            NaN   \n",
       "1283          NaN  2015-02-23 05:37:51 -0800            NaN   \n",
       "11899         NaN  2015-02-24 11:29:54 -0800            NaN   \n",
       "\n",
       "                     user_timezone  \n",
       "77     Mountain Time (US & Canada)  \n",
       "9418                           NaN  \n",
       "8703                           NaN  \n",
       "75     Mountain Time (US & Canada)  \n",
       "13267                          NaN  \n",
       "12962   Eastern Time (US & Canada)  \n",
       "11773                          NaN  \n",
       "9205                         Quito  \n",
       "1283   Mountain Time (US & Canada)  \n",
       "11899                          NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-show\n",
    "\n",
    "tweets = pd.read_csv('dataset/Tweets.csv')\n",
    "\n",
    "#Shuffling the data\n",
    "tweets.sample(frac=1).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following columns in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tweet_id',\n",
       " 'airline_sentiment',\n",
       " 'airline_sentiment_confidence',\n",
       " 'negativereason',\n",
       " 'negativereason_confidence',\n",
       " 'airline',\n",
       " 'airline_sentiment_gold',\n",
       " 'name',\n",
       " 'negativereason_gold',\n",
       " 'retweet_count',\n",
       " 'text',\n",
       " 'tweet_coord',\n",
       " 'tweet_created',\n",
       " 'tweet_location',\n",
       " 'user_timezone']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tweets.columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only concerned with the text and airline_sentiment columns as the purpose of this blog is to walkthrough a basic sentiment analysis pipeline, of course we can make use of other features to extract more information from the data if we wish to.\n",
    "\n",
    "If we look into the text columns, this is what some of them look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @VirginAmerica What @dhepburn said.                                                                                           \n",
       "1    @VirginAmerica plus you've added commercials to the experience... tacky.                                                      \n",
       "2    @VirginAmerica I didn't today... Must mean I need to take another trip!                                                       \n",
       "3    @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse\n",
       "4    @VirginAmerica and it's a really big bad thing about it                                                                       \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['text'][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick division of data is show in the image below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/img_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the data contains a much larger percentage of negative tweets, the other categories still have enough data in them. Hence we don’t have to perform any undersampling/oversampling operations.\n",
    "\n",
    "We also perform some pre processing to clean our data like getting rid of special characters, removing single characters which provide no value to us, eliminating extra spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preprocessing tweets</h2>\n",
    "\n",
    "We perfrom some basic cleaning on our text data using regular expressions.\n",
    "We then split our data into test and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "features = tweets.iloc[:, 10].values\n",
    "labels = tweets.iloc[:, 1].values\n",
    "#preprocessing \n",
    "processed_features = []\n",
    "\n",
    "for sentence in range(0, len(features)):\n",
    "    #Getting rid of special characters\n",
    "    processed_feature = re.sub(r'\\W', ' ', str(features[sentence]))\n",
    "    # remove all single characters\n",
    "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "    # Remove single characters from the start\n",
    "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n",
    "    # Substituting multiple spaces with single space\n",
    "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "    # Removing prefixed 'b'\n",
    "    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
    "    # Converting to Lowercase\n",
    "    processed_feature = processed_feature.lower()\n",
    "    processed_features.append(processed_feature)\n",
    "\n",
    "#Splitting the data \n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping the emotions to numbers for the training and inference step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " united your announcement for pre boarding only addresses mobility my disability requires me to travel with lot of stuff do preboard  1\n"
     ]
    }
   ],
   "source": [
    "#collapse-show\n",
    "\n",
    "d = {\"positive\":2,\"negative\":0,\"neutral\":1}\n",
    "y_train = [d[x] for x in y_train]\n",
    "y_test = [d[x] for x in y_test]\n",
    "\n",
    "print(X_test[10],y_test[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Deep Learning using BERT\n",
    "\n",
    "As mentioned before we would be using BERT and fine tune it to make predictions on our data.\n",
    "\n",
    "The diagram below shows how BERT fits into our workflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/img_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have cleaned our text data, all we have to do is to prepare it for consumption by the model. Depending on which implementation of BERT you want to use this step may differ. But all the approaches require us to encode our labels and tokenize the text. Both these functionalities are generally provided by the libraries offering the BERT implementation.\n",
    "\n",
    "Since we just want to fine tune the model, we don’t have to put in a lot of resources in training. A couple of epochs are good enough to give us good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "20541f718c86fed1131409bcc9ee7736d3ec88ac"
   },
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "\n",
    "folder = 'models/bert'\n",
    "BERT_MODEL = 'uncased_L-12_H-768_A-12'\n",
    "BERT_PRETRAINED_DIR = f'{folder}/uncased_L-12_H-768_A-12'\n",
    "OUTPUT_DIR = f'{folder}/outputs'\n",
    "print(f'>> Model output directory: {OUTPUT_DIR}')\n",
    "print(f'>>  BERT pretrained directory: {BERT_PRETRAINED_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "\n",
    "# keep track of the model name as a mlflow run tag\n",
    "mlflow.set_tag(\"model\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training the model</h2>\n",
    "\n",
    "Now that we have our data ready for use we move on the next step i.e training the model on our data.Since we alrady have the pre-learned weights on the model we can get good results by training the model on our data for just a few epochs.\n",
    "\n",
    "We first start by intializing our model and transforming our data ready for consumption by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1a128c3f38ab69351e34f87977cec04d9d6c9189"
   },
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "\n",
    "def create_examples(lines, set_type, labels=None):\n",
    "#Generate data for the BERT model. We nned data in this format before being fed for training\n",
    "    guid = f'{set_type}'\n",
    "    examples = []\n",
    "    if guid == 'train':\n",
    "        for line, label in zip(lines, labels):\n",
    "            text_a = line\n",
    "            label = str(label)\n",
    "            examples.append(\n",
    "              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "    else:\n",
    "        for line in lines:\n",
    "            text_a = line\n",
    "            label = '0'\n",
    "            examples.append(\n",
    "              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "    return examples\n",
    "\n",
    "# Model Hyper Parameters\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "EVAL_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "WARMUP_PROPORTION = 0.1\n",
    "#We need this to be a little lower thant the max length of tweets we have \n",
    "MAX_SEQ_LENGTH = 50\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 100000 #if you wish to finetune a model on a larger dataset, use larger interval\n",
    "# each checpoint weights about 1,5gb\n",
    "ITERATIONS_PER_LOOP = 100000\n",
    "NUM_TPU_CORES = 8\n",
    "VOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
    "CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
    "INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
    "DO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n",
    "\n",
    "label_list = [str(num) for num in range(3)]\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)\n",
    "train_examples = create_examples(X_train, 'train', labels=y_train)\n",
    "\n",
    "tpu_cluster_resolver = None #Since training will happen on GPU, we won't need a cluster resolver\n",
    "#TPUEstimator also supports training on CPU and GPU. You don't need to define a separate tf.estimator.Estimator.\n",
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "    cluster=tpu_cluster_resolver,\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
    "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "        iterations_per_loop=ITERATIONS_PER_LOOP,\n",
    "        num_shards=NUM_TPU_CORES,\n",
    "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
    "\n",
    "num_train_steps = int(\n",
    "    len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "\n",
    "model_fn = run_classifier.model_fn_builder(\n",
    "    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
    "    num_labels=len(label_list),\n",
    "    init_checkpoint=INIT_CHECKPOINT,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_steps=num_train_steps,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available  \n",
    "    use_one_hot_embeddings=True)\n",
    "\n",
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available \n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    eval_batch_size=EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging parameters into MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "\n",
    "# log parameters before run\n",
    "mlflow.log_param(\"TRAIN_BATCH_SIZE\", TRAIN_BATCH_SIZE)\n",
    "mlflow.log_param(\"EVAL_BATCH_SIZE\", EVAL_BATCH_SIZE)\n",
    "mlflow.log_param(\"LEARNING_RATE\", LEARNING_RATE)\n",
    "mlflow.log_param(\"NUM_TRAIN_EPOCHS\", NUM_TRAIN_EPOCHS)\n",
    "mlflow.log_param(\"WARMUP_PROPORTION\", WARMUP_PROPORTION)\n",
    "mlflow.log_param(\"MAX_SEQ_LENGTH\", MAX_SEQ_LENGTH)\n",
    "mlflow.log_param(\"SAVE_CHECKPOINTS_STEPS\", SAVE_CHECKPOINTS_STEPS)\n",
    "mlflow.log_param(\"ITERATIONS_PER_LOOP\", ITERATIONS_PER_LOOP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training</h2>\n",
    "\n",
    "We now train our model accroding to the previously designed hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e3cb95e4f486c7743add1039f996f501d07109fe"
   },
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "\n",
    "print('Please wait...')\n",
    "train_features = run_classifier.convert_examples_to_features(\n",
    "    train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "print('>> Started training at {} '.format(datetime.datetime.now()))\n",
    "print('  Num examples = {}'.format(len(train_examples)))\n",
    "print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
    "tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
    "train_input_fn = run_classifier.input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=True)\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print('>> Finished training at {}'.format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export and save model variables and protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "\n",
    "serving_model_save_path = 'models/saved_models'\n",
    "\n",
    "\n",
    "def serving_input_receiver_fn():\n",
    "    input_ids = tf.placeholder(dtype=tf.int64, shape=[None, MAX_SEQ_LENGTH], name='input_ids')\n",
    "    input_mask = tf.placeholder(dtype=tf.int64, shape=[None, MAX_SEQ_LENGTH], name='input_mask')\n",
    "    segment_ids = tf.placeholder(dtype=tf.int64, shape=[None, MAX_SEQ_LENGTH], name='segment_ids')\n",
    "    label_ids = tf.placeholder(dtype=tf.int64, shape=[None, ], name='unique_ids')\n",
    "\n",
    "    receive_tensors = {'input_ids': input_ids, 'input_mask': input_mask, 'segment_ids': segment_ids,\n",
    "                       'label_ids': label_ids}\n",
    "    features = {'input_ids': input_ids, 'input_mask': input_mask, 'segment_ids': segment_ids, \"label_ids\": label_ids}\n",
    "    return tf.estimator.export.ServingInputReceiver(features, receive_tensors)\n",
    "\n",
    "estimator._export_to_tpu = False\n",
    "estimator.export_saved_model(serving_model_save_path, serving_input_receiver_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Predicting and Evaluating</h2>\n",
    "\n",
    "Now that our training step is complete in the next steps we will use what our model learned to make predictions on the dataset. We will then evaluate our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2ed42f193d4586b10b58a65ea06c3d51385160bb"
   },
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "\n",
    "def input_fn_builder(features, seq_length, is_training, drop_remainder):\n",
    "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "  all_input_ids = []\n",
    "  all_input_mask = []\n",
    "  all_segment_ids = []\n",
    "  all_label_ids = []\n",
    "\n",
    "  for feature in features:\n",
    "    all_input_ids.append(feature.input_ids)\n",
    "    all_input_mask.append(feature.input_mask)\n",
    "    all_segment_ids.append(feature.segment_ids)\n",
    "    all_label_ids.append(feature.label_id)\n",
    "\n",
    "  def input_fn(params):\n",
    "    \"\"\"The actual input function.\"\"\"\n",
    "    print(params)\n",
    "    batch_size = 500\n",
    "\n",
    "    num_examples = len(features)\n",
    "\n",
    "    d = tf.data.Dataset.from_tensor_slices({\n",
    "        \"input_ids\":\n",
    "            tf.constant(\n",
    "                all_input_ids, shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"input_mask\":\n",
    "            tf.constant(\n",
    "                all_input_mask,\n",
    "                shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"segment_ids\":\n",
    "            tf.constant(\n",
    "                all_segment_ids,\n",
    "                shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"label_ids\":\n",
    "            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n",
    "    })\n",
    "\n",
    "    if is_training:\n",
    "      d = d.repeat()\n",
    "      d = d.shuffle(buffer_size=100)\n",
    "\n",
    "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
    "    return d\n",
    "\n",
    "  return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ddf519b690697635cbd3b47d2ee875feb94fd822"
   },
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "\n",
    "predict_examples = create_examples(X_test, 'test')\n",
    "\n",
    "predict_features = run_classifier.convert_examples_to_features(\n",
    "    predict_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "\n",
    "predict_input_fn = input_fn_builder(\n",
    "    features=predict_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)\n",
    "\n",
    "result = estimator.predict(input_fn=predict_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We get the following results for our model post training :\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/img_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model doesn’t perform well when it comes to neutral sentiment. Possible reason for this could be the general ambiguity which comes in classifying a neutral emotion. Not to say the performance can’t be improved with some tweaking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bb9635b3771ecc148b7dc7a2bf4f5de6a8173445"
   },
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "\n",
    "preds = []\n",
    "for prediction in result:\n",
    "      preds.append(np.argmax(prediction['probabilities']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "de003a2788e27d334232bd515572d73304192fe5"
   },
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "\n",
    "print(\"Accuracy of BERT is:\",accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of BERT is: 0.7990654205607477"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "\n",
    "print(\"F1 Score of BERT is:\",f1_score(y_test, preds, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 Score of BERT is: 0.660558251784892\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "\n",
    "metrics = classification_report(y_test, preds, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "\n",
    "outputframe = DataFrame(dict(sentence = pd.Series(X_train), old_model_label = pd.Series(y_train), pred_label = pd.Series(preds))).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving our output into a csv for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "\n",
    "outputframe.to_csv('output/airline_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"Avg_Precision \", metrics['macro avg']['precision'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"Avg_recall \", metrics['macro avg']['recall']) \n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"Avg_f1-score \",  metrics['macro avg']['f1-score'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"Avg_support \",  metrics['macro avg']['support']) \n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"Accuracy \",  accuracy_score(y_test, preds)) \n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"0_Precision \",  metrics['0']['precision'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"0_recall \",  metrics['0']['recall']) \n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"0_f1-score \",  metrics['0']['f1-score'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"0_support \",  metrics['0']['support']) \n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"1_Precision \",  metrics['1']['precision'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"1_recall \",  metrics['1']['recall']) \n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"1_f1-score \",  metrics['1']['f1-score'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"1_support \",  metrics['1']['support']) \n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"2_Precision \",  metrics['2']['precision'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"2_recall \",  metrics['2']['recall']) \n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"2_f1-score \",  metrics['2']['f1-score'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"2_support \",  metrics['2']['support']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting The Model\n",
    "\n",
    "To better understand and improve our model we need some insights about how decisions are being made. One approach to do this is to use interpretability techniques.\n",
    "\n",
    "For our case, we make use of the library [interpret-text](https://github.com/interpretml/interpret-text). As this library supports only PyTorch we will retrain our model using pytorch. We then use this trained BERT model to run our interpretability algorithm.\n",
    "\n",
    "We use the [Unified Information Explainer](https://www.microsoft.com/en-us/research/publication/towards-a-deep-and-unified-understanding-of-deep-neural-models-in-nlp/) for the task. Let us look at an example.\n",
    "\n",
    "The way the dashboard works is that we can move our slider to pick the ‘n’ most important features according to the model for making a certain prediction. It considers not just the word but it’s surrounding words as well.\n",
    "\n",
    "For the example we are focusing on what the model sees at the 12th and final classification layer.\n",
    "\n",
    "Sentence: @united yup it just happens way too often 5 times in the last 12 months\n",
    "\n",
    "**True Label**: negative\n",
    "\n",
    "**Prediction**: negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/img_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our model focuses the most around the part ‘way too often’ and correctly predicts that it conveys a negative emotion.\n",
    "\n",
    "We can effectively use this tool to look at a subset of sentences and tweak our model by looking at how it processes the sentences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
